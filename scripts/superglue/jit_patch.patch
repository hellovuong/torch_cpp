diff --git a/models/superglue.py b/models/superglue.py
index 5a89b03..a91e422 100644
--- a/models/superglue.py
+++ b/models/superglue.py
@@ -42,7 +42,7 @@
 
 from copy import deepcopy
 from pathlib import Path
-from typing import List, Tuple
+from typing import Dict, List, Tuple
 
 import torch
 from torch import nn
@@ -62,11 +62,18 @@ def MLP(channels: List[int], do_bn: bool = True) -> nn.Module:
     return nn.Sequential(*layers)
 
 
+def _tolist(x: torch.Tensor) -> List[float]:
+    result: List[float] = []
+    for i in x:
+        result.append(i.item())
+    return result
+
+
 def normalize_keypoints(kpts, image_shape):
     """ Normalize keypoints locations based on image image_shape"""
-    _, _, height, width = image_shape
-    one = kpts.new_tensor(1)
-    size = torch.stack([one*width, one*height])[None]
+    # _, _, height, width = image_shape.tolist()
+    _, _, height, width = _tolist(image_shape)
+    size = torch.tensor([[int(height), int(width)]], dtype=torch.float, device=kpts.device)
     center = size / 2
     scaling = size.max(1, keepdim=True).values * 0.7
     return (kpts - center[:, None, :]) / scaling[:, None, :]
@@ -79,6 +86,7 @@ class KeypointEncoder(nn.Module):
         self.encoder = MLP([3] + layers + [feature_dim])
         nn.init.constant_(self.encoder[-1].bias, 0.0)
 
+    @torch.jit.script_method
     def forward(self, kpts, scores):
         inputs = [kpts.transpose(1, 2), scores.unsqueeze(1)]
         return self.encoder(torch.cat(inputs, dim=1))
@@ -101,6 +109,7 @@ class MultiHeadedAttention(nn.Module):
         self.merge = nn.Conv1d(d_model, d_model, kernel_size=1)
         self.proj = nn.ModuleList([deepcopy(self.merge) for _ in range(3)])
 
+    @torch.jit.script_method
     def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:
         batch_dim = query.size(0)
         query, key, value = [l(x).view(batch_dim, self.dim, self.num_heads, -1)
@@ -116,6 +125,7 @@ class AttentionalPropagation(nn.Module):
         self.mlp = MLP([feature_dim*2, feature_dim*2, feature_dim])
         nn.init.constant_(self.mlp[-1].bias, 0.0)
 
+    @torch.jit.script_method
     def forward(self, x: torch.Tensor, source: torch.Tensor) -> torch.Tensor:
         message = self.attn(x, source, source)
         return self.mlp(torch.cat([x, message], dim=1))
@@ -129,9 +139,10 @@ class AttentionalGNN(nn.Module):
             for _ in range(len(layer_names))])
         self.names = layer_names
 
+    @torch.jit.script_method
     def forward(self, desc0: torch.Tensor, desc1: torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor]:
-        for layer, name in zip(self.layers, self.names):
-            if name == 'cross':
+        for i, layer in enumerate(self.layers):
+            if self.names[i] == 'cross':
                 src0, src1 = desc1, desc0
             else:  # if name == 'self':
                 src0, src1 = desc0, desc1
@@ -152,8 +163,7 @@ def log_sinkhorn_iterations(Z: torch.Tensor, log_mu: torch.Tensor, log_nu: torch
 def log_optimal_transport(scores: torch.Tensor, alpha: torch.Tensor, iters: int) -> torch.Tensor:
     """ Perform Differentiable Optimal Transport in Log-space for stability"""
     b, m, n = scores.shape
-    one = scores.new_tensor(1)
-    ms, ns = (m*one).to(scores), (n*one).to(scores)
+    ms, ns = torch.tensor(m).to(scores), torch.tensor(n).to(scores)
 
     bins0 = alpha.expand(b, m, 1)
     bins1 = alpha.expand(b, 1, n)
@@ -173,7 +183,7 @@ def log_optimal_transport(scores: torch.Tensor, alpha: torch.Tensor, iters: int)
 
 
 def arange_like(x, dim: int):
-    return x.new_ones(x.shape[dim]).cumsum(0) - 1  # traceable in 1.1
+    return torch.ones(x.shape[dim], dtype=x.dtype, device=x.device).cumsum(0) - 1
 
 
 class SuperGlue(nn.Module):
@@ -207,27 +217,35 @@ class SuperGlue(nn.Module):
         super().__init__()
         self.config = {**self.default_config, **config}
 
+        self.descriptor_dim = self.config['descriptor_dim']
+        self.weights = self.config['weights']
+        self.keypoint_encoder = self.config['keypoint_encoder']
+        self.GNN_layers = self.config['GNN_layers']
+        self.sinkhorn_iterations = self.config['sinkhorn_iterations']
+        self.match_threshold = self.config['match_threshold']
+
         self.kenc = KeypointEncoder(
-            self.config['descriptor_dim'], self.config['keypoint_encoder'])
+            self.descriptor_dim, self.keypoint_encoder)
 
         self.gnn = AttentionalGNN(
-            feature_dim=self.config['descriptor_dim'], layer_names=self.config['GNN_layers'])
+            feature_dim=self.descriptor_dim, layer_names=self.GNN_layers)
 
         self.final_proj = nn.Conv1d(
-            self.config['descriptor_dim'], self.config['descriptor_dim'],
+            self.descriptor_dim, self.descriptor_dim,
             kernel_size=1, bias=True)
 
         bin_score = torch.nn.Parameter(torch.tensor(1.))
         self.register_parameter('bin_score', bin_score)
 
-        assert self.config['weights'] in ['indoor', 'outdoor']
+        assert self.weights in ['indoor', 'outdoor']
         path = Path(__file__).parent
-        path = path / 'weights/superglue_{}.pth'.format(self.config['weights'])
+        path = path / 'weights/superglue_{}.pth'.format(self.weights)
         self.load_state_dict(torch.load(str(path)))
         print('Loaded SuperGlue model (\"{}\" weights)'.format(
-            self.config['weights']))
+            self.weights))
 
-    def forward(self, data):
+    @torch.jit.script_method
+    def forward(self, data: Dict[str, torch.Tensor]):
         """Run SuperGlue on a pair of keypoints and descriptors"""
         desc0, desc1 = data['descriptors0'], data['descriptors1']
         kpts0, kpts1 = data['keypoints0'], data['keypoints1']
@@ -242,13 +260,17 @@ class SuperGlue(nn.Module):
             }
 
         # Keypoint normalization.
-        kpts0 = normalize_keypoints(kpts0, data['image0'].shape)
-        kpts1 = normalize_keypoints(kpts1, data['image1'].shape)
+        kpts0 = normalize_keypoints(kpts0, data['image0_shape'])
+        kpts1 = normalize_keypoints(kpts1, data['image1_shape'])
 
         # Keypoint MLP encoder.
         desc0 = desc0 + self.kenc(kpts0, data['scores0'])
         desc1 = desc1 + self.kenc(kpts1, data['scores1'])
 
+        match_threshold = self.match_threshold
+        if "match_threshold" in data:
+            match_threshold = _tolist(data["match_threshold"])[0]
+
         # Multi-layer Transformer network.
         desc0, desc1 = self.gnn(desc0, desc1)
 
@@ -257,25 +279,26 @@ class SuperGlue(nn.Module):
 
         # Compute matching descriptor distance.
         scores = torch.einsum('bdn,bdm->bnm', mdesc0, mdesc1)
-        scores = scores / self.config['descriptor_dim']**.5
+        scores = scores / self.descriptor_dim**.5
 
         # Run the optimal transport.
         scores = log_optimal_transport(
             scores, self.bin_score,
-            iters=self.config['sinkhorn_iterations'])
+            iters=self.sinkhorn_iterations)
 
         # Get the matches with score above "match_threshold".
         max0, max1 = scores[:, :-1, :-1].max(2), scores[:, :-1, :-1].max(1)
         indices0, indices1 = max0.indices, max1.indices
         mutual0 = arange_like(indices0, 1)[None] == indices1.gather(1, indices0)
         mutual1 = arange_like(indices1, 1)[None] == indices0.gather(1, indices1)
-        zero = scores.new_tensor(0)
+        zero = torch.tensor(0).to(scores)
         mscores0 = torch.where(mutual0, max0.values.exp(), zero)
         mscores1 = torch.where(mutual1, mscores0.gather(1, indices1), zero)
-        valid0 = mutual0 & (mscores0 > self.config['match_threshold'])
+        # valid0 = mutual0 & (mscores0 > self.match_threshold)
+        valid0 = mutual0 & (mscores0 > match_threshold)
         valid1 = mutual1 & valid0.gather(1, indices1)
-        indices0 = torch.where(valid0, indices0, indices0.new_tensor(-1))
-        indices1 = torch.where(valid1, indices1, indices1.new_tensor(-1))
+        indices0 = torch.where(valid0, indices0, torch.tensor(-1).to(indices0))
+        indices1 = torch.where(valid1, indices1, torch.tensor(-1).to(indices1))
 
         return {
             'matches0': indices0, # use -1 for invalid match
diff --git a/models/superpoint.py b/models/superpoint.py
index b837d93..7af634e 100644
--- a/models/superpoint.py
+++ b/models/superpoint.py
@@ -43,21 +43,23 @@
 from pathlib import Path
 import torch
 from torch import nn
+from typing import Dict, List
+
+def max_pool(x, nms_radius: int):
+    return torch.nn.functional.max_pool2d(
+        x, kernel_size=nms_radius*2+1, stride=1, padding=nms_radius)
+
 
 def simple_nms(scores, nms_radius: int):
     """ Fast Non-maximum suppression to remove nearby points """
     assert(nms_radius >= 0)
 
-    def max_pool(x):
-        return torch.nn.functional.max_pool2d(
-            x, kernel_size=nms_radius*2+1, stride=1, padding=nms_radius)
-
     zeros = torch.zeros_like(scores)
-    max_mask = scores == max_pool(scores)
+    max_mask = scores == max_pool(scores, nms_radius)
     for _ in range(2):
-        supp_mask = max_pool(max_mask.float()) > 0
+        supp_mask = max_pool(max_mask.float(), nms_radius) > 0
         supp_scores = torch.where(supp_mask, zeros, scores)
-        new_max_mask = supp_scores == max_pool(supp_scores)
+        new_max_mask = supp_scores == max_pool(supp_scores, nms_radius)
         max_mask = max_mask | (new_max_mask & (~supp_mask))
     return torch.where(max_mask, scores, zeros)
 
@@ -77,21 +79,27 @@ def top_k_keypoints(keypoints, scores, k: int):
     return keypoints[indices], scores
 
 
-def sample_descriptors(keypoints, descriptors, s: int = 8):
+def sample_descriptors(keypoints, descriptors, s: int = 8, align_corners: bool = True):
     """ Interpolate descriptors at keypoint locations """
     b, c, h, w = descriptors.shape
     keypoints = keypoints - s / 2 + 0.5
     keypoints /= torch.tensor([(w*s - s/2 - 0.5), (h*s - s/2 - 0.5)],
                               ).to(keypoints)[None]
     keypoints = keypoints*2 - 1  # normalize to (-1, 1)
-    args = {'align_corners': True} if torch.__version__ >= '1.3' else {}
     descriptors = torch.nn.functional.grid_sample(
-        descriptors, keypoints.view(b, 1, -1, 2), mode='bilinear', **args)
+        descriptors, keypoints.view(b, 1, -1, 2), mode='bilinear', align_corners=align_corners)
     descriptors = torch.nn.functional.normalize(
-        descriptors.reshape(b, c, -1), p=2, dim=1)
+        descriptors.reshape(b, c, -1), p=2., dim=1)
     return descriptors
 
 
+def _tolist(x: torch.Tensor) -> List[float]:
+    result: List[float] = []
+    for i in x:
+        result.append(i.item())
+    return result
+
+
 class SuperPoint(nn.Module):
     """SuperPoint Convolutional Detector and Descriptor
 
@@ -112,6 +120,12 @@ class SuperPoint(nn.Module):
         super().__init__()
         self.config = {**self.default_config, **config}
 
+        self.descriptor_dim = self.config['descriptor_dim']
+        self.nms_radius = self.config['nms_radius']
+        self.keypoint_threshold = self.config['keypoint_threshold']
+        self.max_keypoints = self.config['max_keypoints']
+        self.remove_borders = self.config['remove_borders']
+
         self.relu = nn.ReLU(inplace=True)
         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
         c1, c2, c3, c4, c5 = 64, 64, 128, 128, 256
@@ -130,22 +144,23 @@ class SuperPoint(nn.Module):
 
         self.convDa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)
         self.convDb = nn.Conv2d(
-            c5, self.config['descriptor_dim'],
+            c5, self.descriptor_dim,
             kernel_size=1, stride=1, padding=0)
 
         path = Path(__file__).parent / 'weights/superpoint_v1.pth'
         self.load_state_dict(torch.load(str(path)))
 
-        mk = self.config['max_keypoints']
+        mk = self.max_keypoints
         if mk == 0 or mk < -1:
             raise ValueError('\"max_keypoints\" must be positive or \"-1\"')
 
         print('Loaded SuperPoint model')
 
-    def forward(self, data):
+    @torch.jit.script_method
+    def forward(self, data: Dict[str, torch.Tensor]):
         """ Compute keypoints, scores, descriptors for image """
         # Shared Encoder
-        x = self.relu(self.conv1a(data['image']))
+        x = self.relu(self.conv1a(data["image"]))
         x = self.relu(self.conv1b(x))
         x = self.pool(x)
         x = self.relu(self.conv2a(x))
@@ -157,6 +172,16 @@ class SuperPoint(nn.Module):
         x = self.relu(self.conv4a(x))
         x = self.relu(self.conv4b(x))
 
+        keypoint_threshold = self.keypoint_threshold
+        remove_borders_value = self.remove_borders
+        nms_radius = self.nms_radius
+        if "keypoint_threshold" in data:
+            keypoint_threshold = _tolist(data["keypoint_threshold"])[0]
+        if "remove_borders" in data:
+            remove_borders_value  = int(_tolist(data["remove_borders"])[0])
+        if "nms_radius" in data:
+            nms_radius = int(_tolist(data["nms_radius"])[0])
+
         # Compute the dense keypoint scores
         cPa = self.relu(self.convPa(x))
         scores = self.convPb(cPa)
@@ -164,39 +189,39 @@ class SuperPoint(nn.Module):
         b, _, h, w = scores.shape
         scores = scores.permute(0, 2, 3, 1).reshape(b, h, w, 8, 8)
         scores = scores.permute(0, 1, 3, 2, 4).reshape(b, h*8, w*8)
-        scores = simple_nms(scores, self.config['nms_radius'])
+        scores = simple_nms(scores, nms_radius)
 
-        # Extract keypoints
-        keypoints = [
-            torch.nonzero(s > self.config['keypoint_threshold'])
-            for s in scores]
-        scores = [s[tuple(k.t())] for s, k in zip(scores, keypoints)]
+        # Compute the dense descriptors
+        cDa = self.relu(self.convDa(x))
+        descriptors = self.convDb(cDa)
+        descriptors = torch.nn.functional.normalize(descriptors, p=2., dim=1)
 
-        # Discard keypoints near the image borders
-        keypoints, scores = list(zip(*[
-            remove_borders(k, s, self.config['remove_borders'], h*8, w*8)
-            for k, s in zip(keypoints, scores)]))
+        keypoints = []
+        scores_out = []
+        descriptors_out = []
+        for i in range(b):
+            # Extract keypoints
+            s = scores[i]
+            k = torch.nonzero(s > keypoint_threshold)
+            s = s[s > keypoint_threshold]
 
-        # Keep the k keypoints with highest score
-        if self.config['max_keypoints'] >= 0:
-            keypoints, scores = list(zip(*[
-                top_k_keypoints(k, s, self.config['max_keypoints'])
-                for k, s in zip(keypoints, scores)]))
+            # Discard keypoints near the image borders
+            k, s = remove_borders(k, s, remove_borders_value, h*8, w*8)
 
-        # Convert (h, w) to (x, y)
-        keypoints = [torch.flip(k, [1]).float() for k in keypoints]
+            # Keep the k keypoints with highest score
+            if self.max_keypoints >= 0:
+                k, s = top_k_keypoints(k, s, self.max_keypoints)
 
-        # Compute the dense descriptors
-        cDa = self.relu(self.convDa(x))
-        descriptors = self.convDb(cDa)
-        descriptors = torch.nn.functional.normalize(descriptors, p=2, dim=1)
+            # Convert (h, w) to (x, y)
+            k = torch.flip(k, [1]).float()
 
-        # Extract descriptors
-        descriptors = [sample_descriptors(k[None], d[None], 8)[0]
-                       for k, d in zip(keypoints, descriptors)]
+            # Extract descriptors
+            descriptors_out.append(sample_descriptors(k.unsqueeze(0), descriptors[i].unsqueeze(0), 8)[0])
+            keypoints.append(k)
+            scores_out.append(s)
 
         return {
             'keypoints': keypoints,
-            'scores': scores,
-            'descriptors': descriptors,
+            'scores': scores_out,
+            'descriptors': descriptors_out,
         }
